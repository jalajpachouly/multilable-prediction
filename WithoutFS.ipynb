{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvEvnSOgpgffdz7IBnDXg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalajpachouly/multilable-prediction/blob/main/WithoutFS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKEZG7Vp_k5A",
        "outputId": "7af915c2-caff-4eb7-d27b-17810d0b565e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing with Unbalanced Data.\n",
            "Label counts in y_train:\n",
            "type_blocker          107\n",
            "type_bug              540\n",
            "type_documentation    907\n",
            "type_enhancement      540\n",
            "dtype: int64\n",
            "\n",
            "Label counts in y_test:\n",
            "type_blocker           27\n",
            "type_bug              136\n",
            "type_documentation    227\n",
            "type_enhancement      136\n",
            "dtype: int64\n",
            "Description length distribution plot saved as 'description_length_distribution_Unbalanced.png'.\n",
            "Class distribution plot saved as 'Unbalanced_class_distribution.png'.\n",
            "Label correlation matrix plot saved as 'label_correlation_matrix_Unbalanced.png'.\n",
            "Label frequency plot saved as 'label_frequency_Unbalanced.png'.\n",
            "Word cloud for 'type_blocker' saved as 'wordcloud_type_blocker.png'.\n",
            "Word cloud for 'type_bug' saved as 'wordcloud_type_bug.png'.\n",
            "Word cloud for 'type_documentation' saved as 'wordcloud_type_documentation.png'.\n",
            "Word cloud for 'type_enhancement' saved as 'wordcloud_type_enhancement.png'.\n",
            "\n",
            "Total unique words collected from word clouds: 93\n",
            "\n",
            "Selected top 50 features based on Chi-Square scores:\n",
            "['spring io' 'issue SPR' 'Spring Security' 'io browse' 's' 'Java'\n",
            " 'JIRA issue' 'jira spring' 'Key Points' 'pull request' 'Spring JIRA'\n",
            " 'Javadoc' 'x' 'Spring' 'due' 'reference documentation' 'Spring Data'\n",
            " 'Spring Integration' 'https jira' 'Spring Framework']\n",
            "\n",
            "Top 20 features have been plotted and saved as 'chi2_features_Unbalanced.png'.\n",
            "\n",
            "===== Cross-Validating MultinomialNB =====\n",
            "Fold 1: Recall = 0.6414, F1-Score = 0.6323\n",
            "Fold 2: Recall = 0.6099, F1-Score = 0.5886\n",
            "Fold 3: Recall = 0.6113, F1-Score = 0.5831\n",
            "Fold 4: Recall = 0.6042, F1-Score = 0.5758\n",
            "Fold 5: Recall = 0.6158, F1-Score = 0.5897\n",
            "Fold 6: Recall = 0.5911, F1-Score = 0.5753\n",
            "Fold 7: Recall = 0.5882, F1-Score = 0.5656\n",
            "Fold 8: Recall = 0.6977, F1-Score = 0.7093\n",
            "Fold 9: Recall = 0.6451, F1-Score = 0.6528\n",
            "Fold 10: Recall = 0.6246, F1-Score = 0.6230\n",
            "\n",
            "===== Cross-Validating LogisticRegression =====\n",
            "Fold 1: Recall = 0.7486, F1-Score = 0.7631\n",
            "Fold 2: Recall = 0.6686, F1-Score = 0.6888\n",
            "Fold 3: Recall = 0.6600, F1-Score = 0.6659\n",
            "Fold 4: Recall = 0.6725, F1-Score = 0.6729\n",
            "Fold 5: Recall = 0.6929, F1-Score = 0.7057\n",
            "Fold 6: Recall = 0.7153, F1-Score = 0.7457\n",
            "Fold 7: Recall = 0.6276, F1-Score = 0.6545\n",
            "Fold 8: Recall = 0.7885, F1-Score = 0.8178\n",
            "Fold 9: Recall = 0.6997, F1-Score = 0.7137\n",
            "Fold 10: Recall = 0.7216, F1-Score = 0.7267\n",
            "\n",
            "===== Cross-Validating RandomForest =====\n",
            "Fold 1: Recall = 0.7357, F1-Score = 0.7474\n",
            "Fold 2: Recall = 0.7634, F1-Score = 0.7657\n",
            "Fold 3: Recall = 0.7598, F1-Score = 0.7652\n",
            "Fold 4: Recall = 0.6976, F1-Score = 0.7261\n",
            "Fold 5: Recall = 0.6953, F1-Score = 0.7180\n",
            "Fold 6: Recall = 0.7697, F1-Score = 0.7670\n",
            "Fold 7: Recall = 0.7239, F1-Score = 0.7468\n",
            "Fold 8: Recall = 0.7715, F1-Score = 0.7973\n",
            "Fold 9: Recall = 0.6798, F1-Score = 0.7053\n",
            "Fold 10: Recall = 0.7235, F1-Score = 0.7390\n",
            "\n",
            "Cross-validation results:\n",
            "                Model    Recall        F1\n",
            "0       MultinomialNB  0.622925  0.609540\n",
            "1  LogisticRegression  0.699536  0.715465\n",
            "2        RandomForest  0.732021  0.747789\n",
            "\n",
            "===== Evaluating MultinomialNB =====\n",
            "Hamming Loss for MultinomialNB: 0.22670250896057348\n",
            "\n",
            "===== Evaluating LogisticRegression =====\n",
            "Hamming Loss for LogisticRegression: 0.22132616487455198\n",
            "\n",
            "===== Evaluating RandomForest =====\n",
            "Hamming Loss for RandomForest: 0.20161290322580644\n",
            "\n",
            "===== Training and Evaluating Deep Learning Model via Cross-Validation =====\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
            "Fold 1: <lambda> Recall = 0.8053, F1-Score = 0.8138\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "Fold 2: <lambda> Recall = 0.8049, F1-Score = 0.7864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78ff76bb1300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x78ff76bb1300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Fold 3: <lambda> Recall = 0.7243, F1-Score = 0.7078\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Fold 4: <lambda> Recall = 0.6795, F1-Score = 0.6956\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "Fold 5: <lambda> Recall = 0.6919, F1-Score = 0.7159\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Fold 6: <lambda> Recall = 0.7165, F1-Score = 0.7288\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "Fold 7: <lambda> Recall = 0.7041, F1-Score = 0.7120\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Fold 8: <lambda> Recall = 0.8110, F1-Score = 0.8196\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Fold 9: <lambda> Recall = 0.6922, F1-Score = 0.6983\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Fold 10: <lambda> Recall = 0.7647, F1-Score = 0.7680\n",
            "\n",
            "Deep Learning Cross-validation results:\n",
            "Recall: 0.7394\n",
            "F1-score: 0.7446\n",
            "\n",
            "===== Training Deep Learning Model on Entire Training Set =====\n",
            "\n",
            "===== Evaluating MLP Model =====\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Hamming Loss for MLP Model: 0.21505376344086022\n",
            "\n",
            "===== Training and Evaluating CNN Model via Cross-Validation =====\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step\n",
            "Fold 1: build_cnn_model Recall = 0.8244, F1-Score = 0.8188\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "Fold 2: build_cnn_model Recall = 0.8597, F1-Score = 0.8310\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step\n",
            "Fold 3: build_cnn_model Recall = 0.6113, F1-Score = 0.6162\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "Fold 4: build_cnn_model Recall = 0.7130, F1-Score = 0.7198\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "Fold 5: build_cnn_model Recall = 0.7699, F1-Score = 0.7732\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
            "Fold 6: build_cnn_model Recall = 0.8284, F1-Score = 0.8019\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "Fold 7: build_cnn_model Recall = 0.7210, F1-Score = 0.7414\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "Fold 8: build_cnn_model Recall = 0.8289, F1-Score = 0.8169\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step\n",
            "Fold 9: build_cnn_model Recall = 0.8247, F1-Score = 0.7868\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "Fold 10: build_cnn_model Recall = 0.7420, F1-Score = 0.7632\n",
            "\n",
            "CNN Cross-validation results:\n",
            "Recall: 0.7723\n",
            "F1-score: 0.7669\n",
            "\n",
            "===== Training CNN Model on Entire Training Set =====\n",
            "Epoch 1/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.3878 - loss: 0.6520 - val_accuracy: 0.4865 - val_loss: 0.4482\n",
            "Epoch 2/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4306 - loss: 0.5480 - val_accuracy: 0.4865 - val_loss: 0.4363\n",
            "Epoch 3/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4664 - loss: 0.4845 - val_accuracy: 0.4910 - val_loss: 0.4268\n",
            "Epoch 4/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5092 - loss: 0.4173 - val_accuracy: 0.4775 - val_loss: 0.4368\n",
            "Epoch 5/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5426 - loss: 0.3734 - val_accuracy: 0.4775 - val_loss: 0.4033\n",
            "Epoch 6/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5476 - loss: 0.2927 - val_accuracy: 0.4910 - val_loss: 0.3894\n",
            "Epoch 7/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5939 - loss: 0.2082 - val_accuracy: 0.4955 - val_loss: 0.3920\n",
            "Epoch 8/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5760 - loss: 0.1289 - val_accuracy: 0.4865 - val_loss: 0.3951\n",
            "Epoch 9/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5966 - loss: 0.0809 - val_accuracy: 0.4955 - val_loss: 0.4149\n",
            "Epoch 10/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5568 - loss: 0.0539 - val_accuracy: 0.4595 - val_loss: 0.4335\n",
            "Epoch 11/20\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6089 - loss: 0.0407 - val_accuracy: 0.4820 - val_loss: 0.4474\n",
            "\n",
            "===== Evaluating CNN Model =====\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n",
            "Hamming Loss for CNN Model: 0.20340501792114696\n",
            "F1 score distribution plot saved as 'f1_score_distribution.png'.\n",
            "All metrics comparison box plot saved as 'all_metrics_comparison_boxplot_Unbalanced.png'.\n",
            "Plot for Multinomial Naive Bayes regression\n",
            "Multinomial Naive Bayes metrics per label plot saved as 'mnb_metrics_per_label_Unbalanced.png'.\n",
            "\n",
            "All visualization processes completed successfully. Plots have been saved.\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Processing with Balanced Data.\n",
            "Label counts in y_train:\n",
            "type_blocker          491\n",
            "type_bug              449\n",
            "type_documentation    509\n",
            "type_enhancement      464\n",
            "dtype: int64\n",
            "\n",
            "Label counts in y_test:\n",
            "type_blocker          123\n",
            "type_bug              112\n",
            "type_documentation    127\n",
            "type_enhancement      116\n",
            "dtype: int64\n",
            "Description length distribution plot saved as 'description_length_distribution_Balanced.png'.\n",
            "Class distribution plot saved as 'Balanced_class_distribution.png'.\n",
            "Label correlation matrix plot saved as 'label_correlation_matrix_Balanced.png'.\n",
            "Label frequency plot saved as 'label_frequency_Balanced.png'.\n",
            "Word cloud for 'type_blocker' saved as 'wordcloud_type_blocker.png'.\n",
            "Word cloud for 'type_bug' saved as 'wordcloud_type_bug.png'.\n",
            "Word cloud for 'type_documentation' saved as 'wordcloud_type_documentation.png'.\n",
            "Word cloud for 'type_enhancement' saved as 'wordcloud_type_enhancement.png'.\n",
            "\n",
            "Total unique words collected from word clouds: 92\n",
            "\n",
            "Selected top 50 features based on Chi-Square scores:\n",
            "['BUILD SNAPSHOT' 'due' 'https jira' 'jira spring' 'Key Points' 'Javadoc'\n",
            " 'x' 'Spring Data' 'Spring Framework' 'reference documentation' 'Spring'\n",
            " 'io browse' 'pull request' 'document describes' 'Juergen Hoeller'\n",
            " 'document discusses' 'e g' 'Spring Batch' 's' 'Spring Integration']\n",
            "\n",
            "Top 20 features have been plotted and saved as 'chi2_features_Balanced.png'.\n",
            "\n",
            "===== Cross-Validating MultinomialNB =====\n",
            "Fold 1: Recall = 0.7934, F1-Score = 0.7991\n",
            "Fold 2: Recall = 0.7438, F1-Score = 0.7518\n",
            "Fold 3: Recall = 0.7426, F1-Score = 0.7626\n",
            "Fold 4: Recall = 0.7901, F1-Score = 0.8064\n",
            "Fold 5: Recall = 0.7736, F1-Score = 0.7880\n",
            "Fold 6: Recall = 0.7411, F1-Score = 0.7577\n",
            "Fold 7: Recall = 0.7567, F1-Score = 0.7551\n",
            "Fold 8: Recall = 0.7466, F1-Score = 0.7575\n",
            "Fold 9: Recall = 0.7544, F1-Score = 0.7823\n",
            "Fold 10: Recall = 0.7322, F1-Score = 0.7521\n",
            "\n",
            "===== Cross-Validating LogisticRegression =====\n",
            "Fold 1: Recall = 0.8153, F1-Score = 0.7910\n",
            "Fold 2: Recall = 0.8234, F1-Score = 0.7907\n",
            "Fold 3: Recall = 0.7852, F1-Score = 0.7701\n",
            "Fold 4: Recall = 0.8211, F1-Score = 0.8101\n",
            "Fold 5: Recall = 0.8290, F1-Score = 0.8136\n",
            "Fold 6: Recall = 0.7790, F1-Score = 0.7664\n",
            "Fold 7: Recall = 0.8603, F1-Score = 0.8055\n",
            "Fold 8: Recall = 0.7881, F1-Score = 0.7711\n",
            "Fold 9: Recall = 0.8398, F1-Score = 0.8257\n",
            "Fold 10: Recall = 0.8177, F1-Score = 0.8016\n",
            "\n",
            "===== Cross-Validating RandomForest =====\n",
            "Fold 1: Recall = 0.8862, F1-Score = 0.8812\n",
            "Fold 2: Recall = 0.8874, F1-Score = 0.8779\n",
            "Fold 3: Recall = 0.8882, F1-Score = 0.8845\n",
            "Fold 4: Recall = 0.8706, F1-Score = 0.8723\n",
            "Fold 5: Recall = 0.8644, F1-Score = 0.8584\n",
            "Fold 6: Recall = 0.8759, F1-Score = 0.8753\n",
            "Fold 7: Recall = 0.8871, F1-Score = 0.8645\n",
            "Fold 8: Recall = 0.8767, F1-Score = 0.8688\n",
            "Fold 9: Recall = 0.8982, F1-Score = 0.8951\n",
            "Fold 10: Recall = 0.8828, F1-Score = 0.8824\n",
            "\n",
            "Cross-validation results:\n",
            "                Model    Recall        F1\n",
            "0       MultinomialNB  0.757447  0.771248\n",
            "1  LogisticRegression  0.815881  0.794580\n",
            "2        RandomForest  0.881734  0.876043\n",
            "\n",
            "===== Evaluating MultinomialNB =====\n",
            "Hamming Loss for MultinomialNB: 0.20402298850574713\n",
            "\n",
            "===== Evaluating LogisticRegression =====\n",
            "Hamming Loss for LogisticRegression: 0.18199233716475097\n",
            "\n",
            "===== Evaluating RandomForest =====\n",
            "Hamming Loss for RandomForest: 0.09386973180076628\n",
            "\n",
            "===== Training and Evaluating Deep Learning Model via Cross-Validation =====\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Fold 1: <lambda> Recall = 0.8266, F1-Score = 0.8069\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Fold 2: <lambda> Recall = 0.8143, F1-Score = 0.8078\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "Fold 3: <lambda> Recall = 0.8275, F1-Score = 0.8259\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Fold 4: <lambda> Recall = 0.8271, F1-Score = 0.8295\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "Fold 5: <lambda> Recall = 0.8129, F1-Score = 0.8147\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Fold 6: <lambda> Recall = 0.7895, F1-Score = 0.7973\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Fold 7: <lambda> Recall = 0.8544, F1-Score = 0.8067\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Fold 8: <lambda> Recall = 0.8162, F1-Score = 0.8025\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Fold 9: <lambda> Recall = 0.8164, F1-Score = 0.8163\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Fold 10: <lambda> Recall = 0.8178, F1-Score = 0.8330\n",
            "\n",
            "Deep Learning Cross-validation results:\n",
            "Recall: 0.8203\n",
            "F1-score: 0.8141\n",
            "\n",
            "===== Training Deep Learning Model on Entire Training Set =====\n",
            "\n",
            "===== Evaluating MLP Model =====\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Hamming Loss for MLP Model: 0.20402298850574713\n",
            "\n",
            "===== Training and Evaluating CNN Model via Cross-Validation =====\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "Fold 1: build_cnn_model Recall = 0.9726, F1-Score = 0.9602\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step\n",
            "Fold 2: build_cnn_model Recall = 0.9466, F1-Score = 0.9391\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "Fold 3: build_cnn_model Recall = 0.9351, F1-Score = 0.9350\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
            "Fold 4: build_cnn_model Recall = 0.9729, F1-Score = 0.9730\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "Fold 5: build_cnn_model Recall = 0.9422, F1-Score = 0.9420\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "Fold 6: build_cnn_model Recall = 0.9733, F1-Score = 0.9760\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "Fold 7: build_cnn_model Recall = 0.9402, F1-Score = 0.9266\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "Fold 8: build_cnn_model Recall = 0.9193, F1-Score = 0.9249\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "Fold 9: build_cnn_model Recall = 0.9839, F1-Score = 0.9737\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "Fold 10: build_cnn_model Recall = 0.9793, F1-Score = 0.9769\n",
            "\n",
            "CNN Cross-validation results:\n",
            "Recall: 0.9565\n",
            "F1-score: 0.9527\n",
            "\n",
            "===== Training CNN Model on Entire Training Set =====\n",
            "Epoch 1/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.4305 - loss: 0.6714 - val_accuracy: 0.0433 - val_loss: 0.9321\n",
            "Epoch 2/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6849 - loss: 0.5888 - val_accuracy: 0.2500 - val_loss: 0.7767\n",
            "Epoch 3/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7361 - loss: 0.4219 - val_accuracy: 0.8654 - val_loss: 0.6935\n",
            "Epoch 4/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6063 - loss: 0.2766 - val_accuracy: 0.9087 - val_loss: 0.5693\n",
            "Epoch 5/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6160 - loss: 0.1916 - val_accuracy: 0.7596 - val_loss: 0.2717\n",
            "Epoch 6/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6245 - loss: 0.1103 - val_accuracy: 0.6250 - val_loss: 0.1585\n",
            "Epoch 7/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6364 - loss: 0.0692 - val_accuracy: 0.6010 - val_loss: 0.1361\n",
            "Epoch 8/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6736 - loss: 0.0510 - val_accuracy: 0.5144 - val_loss: 0.1368\n",
            "Epoch 9/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6679 - loss: 0.0409 - val_accuracy: 0.5288 - val_loss: 0.1497\n",
            "Epoch 10/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5992 - loss: 0.0318 - val_accuracy: 0.4471 - val_loss: 0.1635\n",
            "Epoch 11/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6395 - loss: 0.0284 - val_accuracy: 0.5865 - val_loss: 0.1742\n",
            "Epoch 12/20\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6455 - loss: 0.0228 - val_accuracy: 0.6202 - val_loss: 0.1870\n",
            "\n",
            "===== Evaluating CNN Model =====\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step\n",
            "Hamming Loss for CNN Model: 0.14080459770114942\n",
            "F1 score distribution plot saved as 'f1_score_distribution.png'.\n",
            "All metrics comparison box plot saved as 'all_metrics_comparison_boxplot_Balanced.png'.\n",
            "Plot for Multinomial Naive Bayes regression\n",
            "Multinomial Naive Bayes metrics per label plot saved as 'mnb_metrics_per_label_Balanced.png'.\n",
            "\n",
            "All visualization processes completed successfully. Plots have been saved.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Multi-Label Classification for Bug Reports\n",
        "\n",
        "This script performs multi-label classification on bug reports using several machine learning models,\n",
        "including traditional classifiers and deep learning models like MLP and CNN. It includes functions\n",
        "for data loading, preprocessing, visualization, model training, evaluation, and result visualization.\n",
        "\n",
        "Author: Your Name\n",
        "Date: October 2023\n",
        "\"\"\"\n",
        "\n",
        "# ====================================\n",
        "# Import Libraries\n",
        "# ====================================\n",
        "\n",
        "# Data manipulation and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# NLP and text processing\n",
        "import nltk\n",
        "\n",
        "# Machine learning and evaluation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import recall_score, f1_score, hamming_loss\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Multi-label stratification\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit, MultilabelStratifiedKFold\n",
        "\n",
        "# Deep learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Transformer models\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "# Optimization\n",
        "from scipy.optimize import nnls\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define global labels\n",
        "LABELS = [\"type_blocker\", \"type_bug\", \"type_documentation\", \"type_enhancement\"]\n",
        "\n",
        "# ====================================\n",
        "# Data Loading and Preparation\n",
        "# ====================================\n",
        "\n",
        "def build_conditional_prob_matrix(df, labels):\n",
        "    \"\"\"\n",
        "    Build a conditional probability matrix for label co-occurrence.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): DataFrame containing the labels.\n",
        "    - labels (list of str): List of label column names.\n",
        "\n",
        "    Returns:\n",
        "    - cooc_norm (np.ndarray): Normalized co-occurrence matrix.\n",
        "    \"\"\"\n",
        "    cooc = df[labels].values.T.dot(df[labels].values)\n",
        "    cooc_norm = cooc.copy().astype(np.float32)\n",
        "    for i in range(cooc_norm.shape[0]):\n",
        "        cooc_norm[:, i] /= cooc[i, i]\n",
        "    return cooc_norm\n",
        "\n",
        "def nnls_sample(df, labels, target_count, cond_prob):\n",
        "    \"\"\"\n",
        "    Perform stratified sampling to balance the dataset based on label co-occurrence.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): DataFrame containing the data and labels.\n",
        "    - labels (list of str): List of label column names.\n",
        "    - target_count (int): Desired number of samples per label.\n",
        "    - cond_prob (np.ndarray): Conditional probability matrix from build_conditional_prob_matrix.\n",
        "\n",
        "    Returns:\n",
        "    - sampled_df (pd.DataFrame): The resampled DataFrame.\n",
        "    \"\"\"\n",
        "    target_counts = np.array([target_count for _ in labels])\n",
        "    optimal_samples, residuals = nnls(cond_prob, target_counts)\n",
        "    optimal_samples = np.ceil(optimal_samples).astype(np.int32)\n",
        "    df_subs = []\n",
        "    for i, label in enumerate(labels):\n",
        "        sub_df = df[df[label] == 1]\n",
        "        df_subs.append(sub_df.sample(optimal_samples[i],\n",
        "                                     replace=len(sub_df) < optimal_samples[i]))\n",
        "    sampled_df = pd.concat(df_subs)\n",
        "    return sampled_df\n",
        "\n",
        "def load_data(csv_path: str):\n",
        "    \"\"\"\n",
        "    Load the dataset from a CSV file and split it into training and testing sets using stratified splitting.\n",
        "\n",
        "    Parameters:\n",
        "    - csv_path (str): Path to the CSV file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - X_train (pd.DataFrame): Training set features.\n",
        "    - X_test (pd.DataFrame): Testing set features.\n",
        "    - y_train (pd.DataFrame): Training set labels.\n",
        "    - y_test (pd.DataFrame): Testing set labels.\n",
        "    \"\"\"\n",
        "    # Load the dataset from a CSV file\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    # Check if required columns exist\n",
        "    required_columns = ['report', 'type_blocker', 'type_bug', 'type_documentation', 'type_enhancement']\n",
        "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"The following required columns are missing in the dataset: {missing_columns}\")\n",
        "\n",
        "    # Feature column (text data)\n",
        "    X = data[['report']]\n",
        "\n",
        "    # Label columns (multi-label targets)\n",
        "    y = data[['type_blocker', 'type_bug', 'type_documentation', 'type_enhancement']]\n",
        "\n",
        "    # Initialize the stratified shuffle split\n",
        "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Perform the split\n",
        "    for train_index, test_index in msss.split(X, y):\n",
        "        X_train = X.iloc[train_index].reset_index(drop=True)\n",
        "        X_test = X.iloc[test_index].reset_index(drop=True)\n",
        "        y_train = y.iloc[train_index].reset_index(drop=True)\n",
        "        y_test = y.iloc[test_index].reset_index(drop=True)\n",
        "\n",
        "    # Print label counts to check for class imbalance\n",
        "    print(\"Label counts in y_train:\")\n",
        "    print(y_train.sum())\n",
        "    print(\"\\nLabel counts in y_test:\")\n",
        "    print(y_test.sum())\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def load_data_balanced(csv_path: str):\n",
        "    \"\"\"\n",
        "    Load the dataset from a CSV file, balance it, and split into training and testing sets.\n",
        "\n",
        "    Parameters:\n",
        "    - csv_path (str): Path to the CSV file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    - X_train (pd.DataFrame): Training set features.\n",
        "    - X_test (pd.DataFrame): Testing set features.\n",
        "    - y_train (pd.DataFrame): Training set labels.\n",
        "    - y_test (pd.DataFrame): Testing set labels.\n",
        "    \"\"\"\n",
        "    # Load the dataset from a CSV file\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    # Build conditional probability matrix and perform NNLS sampling\n",
        "    cooc_norm = build_conditional_prob_matrix(data, LABELS)\n",
        "    resampled_df = nnls_sample(data, LABELS, 600, cooc_norm)\n",
        "    data = resampled_df\n",
        "\n",
        "    # Check if required columns exist\n",
        "    required_columns = ['report', 'type_blocker', 'type_bug', 'type_documentation', 'type_enhancement']\n",
        "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"The following required columns are missing in the dataset: {missing_columns}\")\n",
        "\n",
        "    # Feature column (text data)\n",
        "    X = data[['report']]\n",
        "\n",
        "    # Label columns (multi-label targets)\n",
        "    y = data[['type_blocker', 'type_bug', 'type_documentation', 'type_enhancement']]\n",
        "\n",
        "    # Initialize the stratified shuffle split\n",
        "    msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Perform the split\n",
        "    for train_index, test_index in msss.split(X, y):\n",
        "        X_train = X.iloc[train_index].reset_index(drop=True)\n",
        "        X_test = X.iloc[test_index].reset_index(drop=True)\n",
        "        y_train = y.iloc[train_index].reset_index(drop=True)\n",
        "        y_test = y.iloc[test_index].reset_index(drop=True)\n",
        "\n",
        "    # Print label counts to check for class imbalance\n",
        "    print(\"Label counts in y_train:\")\n",
        "    print(y_train.sum())\n",
        "    print(\"\\nLabel counts in y_test:\")\n",
        "    print(y_test.sum())\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def prepare_data(X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.DataFrame, top_k=50, vocabulary=None):\n",
        "    \"\"\"\n",
        "    Convert text data to TF-IDF features and perform feature selection using the Chi-Square test.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (pd.DataFrame): Training set features.\n",
        "    - X_test (pd.DataFrame): Testing set features.\n",
        "    - y_train (pd.DataFrame): Training set labels.\n",
        "    - top_k (int): Number of top features to select based on Chi-Square scores.\n",
        "    - vocabulary (list or None): Predefined vocabulary to use for TF-IDF vectorizer.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_selected (sparse matrix): Selected training set features.\n",
        "    - X_test_selected (sparse matrix): Selected testing set features.\n",
        "    - selected_features (np.array): Names of the selected features.\n",
        "    - chi2_scores_max (np.array): Maximum Chi-Square scores for each feature.\n",
        "    - vector (TfidfVectorizer): The fitted TF-IDF vectorizer.\n",
        "    \"\"\"\n",
        "    # Initialize the TF-IDF vectorizer\n",
        "    vector = TfidfVectorizer(\n",
        "        ngram_range=(1, 1),\n",
        "        analyzer='word',\n",
        "        stop_words='english',\n",
        "        strip_accents='unicode',\n",
        "        use_idf=True,\n",
        "        min_df=1,\n",
        "        vocabulary=vocabulary  # Predefined vocabulary\n",
        "    )\n",
        "\n",
        "    # Apply TF-IDF on the 'report' column\n",
        "    X_train_tfidf = vector.fit_transform(X_train['report'])\n",
        "    X_test_tfidf = vector.transform(X_test['report'])\n",
        "\n",
        "    # Feature selection using Chi-Square test\n",
        "    chi2_scores = []\n",
        "    for i in range(y_train.shape[1]):\n",
        "        chi2_score_values, p_value = chi2(X_train_tfidf, y_train.iloc[:, i])\n",
        "        chi2_scores.append(chi2_score_values)\n",
        "\n",
        "    # Aggregate Chi-Square scores across labels by taking the maximum score for each feature\n",
        "    chi2_scores_max = np.max(np.array(chi2_scores), axis=0)\n",
        "\n",
        "    # Select top K features based on Chi-Square scores\n",
        "    if top_k > len(chi2_scores_max):\n",
        "        top_k = len(chi2_scores_max)\n",
        "    selected_indices = np.argsort(chi2_scores_max)[::-1][:top_k]\n",
        "    selected_indices = selected_indices.astype(int)\n",
        "    X_train_selected = X_train_tfidf[:, selected_indices]\n",
        "    X_test_selected = X_test_tfidf[:, selected_indices]\n",
        "\n",
        "    # Retrieve selected feature names\n",
        "    selected_features = np.array(vector.get_feature_names_out())[selected_indices]\n",
        "    print(f\"\\nSelected top {top_k} features based on Chi-Square scores:\")\n",
        "    print(selected_features[:20])\n",
        "\n",
        "    return X_train_selected, X_test_selected, selected_features, chi2_scores_max, vector\n",
        "\n",
        "def prepare_data_for_deep_learning(X_train_texts, X_test_texts, max_words=5000, max_len=100):\n",
        "    \"\"\"\n",
        "    Tokenize and pad text data for deep learning models.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train_texts: Training text data.\n",
        "    - X_test_texts: Testing text data.\n",
        "    - max_words: Maximum number of words to consider in the vocabulary.\n",
        "    - max_len: Maximum length of sequences after padding.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_pad: Padded training sequences.\n",
        "    - X_test_pad: Padded testing sequences.\n",
        "    - tokenizer: Fitted Keras tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(num_words=max_words, oov_token='')\n",
        "    tokenizer.fit_on_texts(X_train_texts)\n",
        "\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
        "\n",
        "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    return X_train_pad, X_test_pad, tokenizer\n",
        "\n",
        "def prepare_data_for_transformer(X_texts, tokenizer, max_len=100):\n",
        "    \"\"\"\n",
        "    Tokenize text data for the Transformer model.\n",
        "\n",
        "    Parameters:\n",
        "    - X_texts: List or Series of text data.\n",
        "    - tokenizer: Pretrained tokenizer from Hugging Face.\n",
        "    - max_len: Maximum sequence length.\n",
        "\n",
        "    Returns:\n",
        "    - input_ids: Token IDs for each text.\n",
        "    - attention_masks: Attention masks for each text.\n",
        "    \"\"\"\n",
        "    encodings = tokenizer.batch_encode_plus(\n",
        "        X_texts.tolist(),\n",
        "        max_length=max_len,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    return encodings['input_ids'], encodings['attention_mask']\n",
        "\n",
        "# ====================================\n",
        "# Visualization Functions\n",
        "# ====================================\n",
        "\n",
        "def visualize_description_length(X_train: pd.DataFrame, data_type: str):\n",
        "    \"\"\"\n",
        "    Visualize the distribution of description lengths in the given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (pd.DataFrame): DataFrame containing the 'report' column.\n",
        "    - data_type (str): String indicating the type of data (e.g., 'Balanced').\n",
        "    \"\"\"\n",
        "    sns.set(style=\"darkgrid\")\n",
        "    X_train['report'] = X_train['report'].astype(str)\n",
        "    description_len = X_train['report'].str.len()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(description_len, kde=False, bins=20, color=\"steelblue\")\n",
        "    plt.xlabel('Description Length')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Description Length Distribution')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'description_length_distribution_{data_type}.png')\n",
        "    plt.close()\n",
        "    print(f\"Description length distribution plot saved as 'description_length_distribution_{data_type}.png'.\")\n",
        "\n",
        "def visualize_class_distribution(y_train: pd.DataFrame, y_test: pd.DataFrame, data_type: str, save_path='class_distribution.png'):\n",
        "    \"\"\"\n",
        "    Visualize the distribution of classes within each label for both training and test datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - y_train (pd.DataFrame): Training set labels.\n",
        "    - y_test (pd.DataFrame): Testing set labels.\n",
        "    - data_type (str): String indicating the type of data (e.g., 'Balanced').\n",
        "    - save_path (str): Filename for saving the plot.\n",
        "    \"\"\"\n",
        "    labels = y_train.columns.tolist()\n",
        "    bar_width = 0.2\n",
        "    bars1 = [sum(y_train[label] == 1) for label in labels]\n",
        "    bars2 = [sum(y_train[label] == 0) for label in labels]\n",
        "    bars3 = [sum(y_test[label] == 1) for label in labels]\n",
        "    bars4 = [sum(y_test[label] == 0) for label in labels]\n",
        "\n",
        "    r1 = np.arange(len(bars1))\n",
        "    r2 = [x + bar_width for x in r1]\n",
        "    r3 = [x + bar_width for x in r2]\n",
        "    r4 = [x + bar_width for x in r3]\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.bar(r1, bars1, color='steelblue', width=bar_width, label='Train Labeled = 1')\n",
        "    plt.bar(r2, bars2, color='lightsteelblue', width=bar_width, label='Train Labeled = 0')\n",
        "    plt.bar(r3, bars3, color='darkorange', width=bar_width, label='Test Labeled = 1')\n",
        "    plt.bar(r4, bars4, color='navajowhite', width=bar_width, label='Test Labeled = 0')\n",
        "\n",
        "    plt.xlabel('Labels', fontweight='bold')\n",
        "    plt.xticks([r + bar_width * 1.5 for r in range(len(bars1))], labels, rotation=45)\n",
        "    plt.legend()\n",
        "    plt.title('Distribution of Classes within Each Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{data_type}_{save_path}')\n",
        "    plt.close()\n",
        "    print(f\"Class distribution plot saved as '{data_type}_{save_path}'.\")\n",
        "\n",
        "def visualize_word_cloud(X_train: pd.DataFrame, y_train: pd.DataFrame, token: str, max_words=50):\n",
        "    \"\"\"\n",
        "    Visualize the most common words contributing to the token and return top words.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (pd.DataFrame): Training set features.\n",
        "    - y_train (pd.DataFrame): Training set labels.\n",
        "    - token (str): The label to visualize word cloud for.\n",
        "    - max_words (int): Number of top words to return.\n",
        "\n",
        "    Returns:\n",
        "    - top_words (list): List of top words based on frequency.\n",
        "    \"\"\"\n",
        "    description_context = X_train.join(y_train)\n",
        "    description_context = description_context[description_context[token] == 1]\n",
        "    if description_context.empty:\n",
        "        print(f\"No instances for label '{token}'; skipping word cloud.\")\n",
        "        return []\n",
        "\n",
        "    description_text = description_context['report']\n",
        "    combined_text = ' '.join(description_text)\n",
        "    wordcloud = WordCloud(width=1600, height=800, max_font_size=200, background_color='white').generate(combined_text)\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.imshow(wordcloud.recolor(colormap=\"Blues\"), interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Most Common Words Associated with '{token}' Defects\", size=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'wordcloud_{token}.png')\n",
        "    plt.close()\n",
        "    print(f\"Word cloud for '{token}' saved as 'wordcloud_{token}.png'.\")\n",
        "\n",
        "    # Extract word frequencies from the word cloud\n",
        "    word_freq = wordcloud.words_\n",
        "    # Sort words by frequency\n",
        "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    # Get top N words\n",
        "    top_words = [word for word, freq in sorted_words[:max_words]]\n",
        "    return top_words\n",
        "\n",
        "def visualize_f1_scores(methods: pd.DataFrame,data_type: str):\n",
        "    \"\"\"\n",
        "    Visualize F1 score results through a box plot with jittered points.\n",
        "\n",
        "    Parameters:\n",
        "    - methods (pd.DataFrame): DataFrame containing evaluation metrics.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    ax = sns.boxplot(x='Model', y='F1', data=methods, palette=\"Blues\")\n",
        "    sns.stripplot(x='Model', y='F1', data=methods, size=8, jitter=True,\n",
        "                  edgecolor=\"gray\", linewidth=2, palette=\"Blues\")\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=20)\n",
        "    plt.title('F1 Score Distribution by Model')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.xlabel('Model')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'f1_score_distribution_{data_type}.png')\n",
        "    plt.close()\n",
        "    print(\"F1 score distribution plot saved as 'f1_score_distribution.png'.\")\n",
        "\n",
        "def visualize_all_metrics_boxplot(methods: pd.DataFrame, data_type: str):\n",
        "    \"\"\"\n",
        "    Create a box plot comparing Recall, F1-score, and Hamming Loss across all models.\n",
        "\n",
        "    Parameters:\n",
        "    - methods (pd.DataFrame): DataFrame containing evaluation metrics.\n",
        "    - data_type (str): String indicating the type of data (e.g., 'Balanced').\n",
        "    \"\"\"\n",
        "    # Melt the DataFrame to have Metrics in a single column\n",
        "    metrics_melted = methods.melt(id_vars=['Model', 'Label'], value_vars=['Recall', 'F1', 'Hamming Loss'],\n",
        "                                  var_name='Metric', value_name='Score')\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.boxplot(x='Metric', y='Score', hue='Model', data=metrics_melted, palette=\"Set2\")\n",
        "    sns.stripplot(x='Metric', y='Score', hue='Model', data=metrics_melted, dodge=True,\n",
        "                  color='gray', alpha=0.6, size=5, jitter=True)\n",
        "    plt.title('Comparison of Evaluation Metrics Across Models')\n",
        "    plt.xlabel('Evaluation Metric')\n",
        "    plt.ylabel('Score')\n",
        "    # Handle legends to avoid duplication\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    plt.legend(by_label.values(), by_label.keys(), title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'all_metrics_comparison_boxplot_{data_type}.png')\n",
        "    plt.close()\n",
        "    print(f\"All metrics comparison box plot saved as 'all_metrics_comparison_boxplot_{data_type}.png'.\")\n",
        "\n",
        "def visualize_nb_metrics(methods: pd.DataFrame, data_type: str):\n",
        "    \"\"\"\n",
        "    Create a bar graph of F1 and Recall across each label for Multinomial Naive Bayes.\n",
        "\n",
        "    Parameters:\n",
        "    - methods (pd.DataFrame): DataFrame containing evaluation metrics.\n",
        "    - data_type (str): String indicating the type of data (e.g., 'Balanced').\n",
        "    \"\"\"\n",
        "    print(\"Plot for Multinomial Naive Bayes regression\")\n",
        "    m2 = methods[methods.Model == 'MultinomialNB'].copy()\n",
        "    if m2.empty:\n",
        "        print(\"No data available for MultinomialNB metrics; skipping plot.\")\n",
        "        return\n",
        "    m2.set_index([\"Label\"], inplace=True)\n",
        "    ax = m2[['Recall', 'F1']].plot(figsize=(16, 8), kind='bar', title='Multinomial Naive Bayes Metrics by Label',\n",
        "                                   rot=60, ylim=(0.0, 1), colormap='tab10')\n",
        "    plt.ylabel('Score')\n",
        "    plt.xlabel('Labels')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'mnb_metrics_per_label_{data_type}.png')\n",
        "    plt.close()\n",
        "    print(f\"Multinomial Naive Bayes metrics per label plot saved as 'mnb_metrics_per_label_{data_type}.png'.\")\n",
        "\n",
        "def visualize_correlation_matrix(y_train: pd.DataFrame, data_type: str):\n",
        "    \"\"\"\n",
        "    Visualize the cross-correlation matrix across labels in the given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - y_train (pd.DataFrame): Training set labels.\n",
        "    - data_type (str): String indicating the type of data (e.g., 'Balanced').\n",
        "    \"\"\"\n",
        "    label_columns = y_train.columns.tolist()\n",
        "    if not label_columns:\n",
        "        print(\"No label columns found in the DataFrame for correlation matrix; skipping plot.\")\n",
        "        return\n",
        "\n",
        "    train_corr = y_train.copy()\n",
        "    if train_corr.empty:\n",
        "        print(\"No data available for correlation matrix; skipping plot.\")\n",
        "        return\n",
        "\n",
        "    corr = train_corr.corr()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, cmap=\"Blues\", fmt=\".2f\",\n",
        "                xticklabels=corr.columns.values, yticklabels=corr.columns.values)\n",
        "    plt.title('Correlation Matrix of Labels')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'label_correlation_matrix_{data_type}.png')\n",
        "    plt.close()\n",
        "    print(f\"Label correlation matrix plot saved as 'label_correlation_matrix_{data_type}.png'.\")\n",
        "\n",
        "def visualize_label_frequency(y_train: pd.DataFrame, data_type: str):\n",
        "    \"\"\"\n",
        "    Visualize the frequency of specified labels in the given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - y_train (pd.DataFrame): Training set labels.\n",
        "    - data_type (str): String indicating the type of data (e.g., 'Balanced').\n",
        "    \"\"\"\n",
        "    labels = [\"type_bug\", \"type_documentation\", \"type_enhancement\"]\n",
        "    # Filter labels that exist in the DataFrame\n",
        "    present_labels = [label for label in labels if label in y_train.columns]\n",
        "    if not present_labels:\n",
        "        print(\"No specified labels found in the DataFrame; skipping label frequency plot.\")\n",
        "        return\n",
        "\n",
        "    label_count = y_train[present_labels].sum()\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=label_count.index, y=label_count.values, color=\"steelblue\")\n",
        "    plt.title('Labels Frequency')\n",
        "    plt.xlabel('Labels')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'label_frequency_{data_type}.png')\n",
        "    plt.close()\n",
        "    print(f\"Label frequency plot saved as 'label_frequency_{data_type}.png'.\")\n",
        "\n",
        "def plot_top_features(selected_features, chi2_scores_max, data_type: str, top_k_plot=20):\n",
        "    \"\"\"\n",
        "    Plot the top features based on Chi-Square scores.\n",
        "\n",
        "    Parameters:\n",
        "    - selected_features (np.array): Array of selected feature names.\n",
        "    - chi2_scores_max (np.array): Maximum Chi-Square scores for each feature.\n",
        "    - data_type (str): String indicating the type of data (e.g., 'Balanced').\n",
        "    - top_k_plot (int): Number of top features to plot.\n",
        "    \"\"\"\n",
        "    top_features = selected_features[:top_k_plot]\n",
        "    selected_indices = np.argsort(chi2_scores_max)[::-1][:top_k_plot]\n",
        "    selected_indices = selected_indices.astype(int)\n",
        "    top_scores = chi2_scores_max[selected_indices[:top_k_plot]]\n",
        "    # Plot the top features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x=top_scores, y=top_features)\n",
        "    plt.title(f'Top {top_k_plot} Features Based on Chi-Square Scores')\n",
        "    plt.xlabel('Chi-Square Score')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'chi2_features_{data_type}.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\nTop {top_k_plot} features have been plotted and saved as 'chi2_features_{data_type}.png'.\")\n",
        "\n",
        "# ====================================\n",
        "# Model Training and Evaluation\n",
        "# ====================================\n",
        "\n",
        "def cross_validation_score_multilabel(classifier, X, y, n_splits=10):\n",
        "    \"\"\"\n",
        "    Perform cross-validation and compute average Recall and F1-score.\n",
        "\n",
        "    Parameters:\n",
        "    - classifier: The classifier to evaluate.\n",
        "    - X (sparse matrix or ndarray): Feature matrix.\n",
        "    - y (ndarray): Label matrix.\n",
        "    - n_splits (int): Number of cross-validation splits.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary containing average Recall and F1-score.\n",
        "    \"\"\"\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(mskf.split(X, y), 1):\n",
        "        X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
        "        y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
        "\n",
        "        classifier.fit(X_train_cv, y_train_cv)\n",
        "        y_pred_cv = classifier.predict(X_test_cv)\n",
        "\n",
        "        # Compute recall and F1 score\n",
        "        recall = recall_score(y_test_cv, y_pred_cv, average='macro', zero_division=0)\n",
        "        f1 = f1_score(y_test_cv, y_pred_cv, average='macro', zero_division=0)\n",
        "\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold {fold}: Recall = {recall:.4f}, F1-Score = {f1:.4f}\")\n",
        "\n",
        "    avg_recall = np.mean(recall_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    return {'Recall': avg_recall, 'F1': avg_f1}\n",
        "\n",
        "def cross_validation_score_deep_learning(model_builder, X, y, n_splits=10, epochs=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    Perform cross-validation for a Deep Learning model and compute average Recall and F1-score.\n",
        "\n",
        "    Parameters:\n",
        "    - model_builder: Function to build the model.\n",
        "    - X (ndarray): Feature matrix.\n",
        "    - y (ndarray): Label matrix.\n",
        "    - n_splits (int): Number of cross-validation splits.\n",
        "    - epochs (int): Number of training epochs.\n",
        "    - batch_size (int): Batch size for training.\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary containing average Recall and F1-score.\n",
        "    \"\"\"\n",
        "    mskf = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(mskf.split(X, y), 1):\n",
        "        X_train_cv, X_test_cv = X[train_index], X[test_index]\n",
        "        y_train_cv, y_test_cv = y[train_index], y[test_index]\n",
        "\n",
        "        model = model_builder()\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "        model.fit(\n",
        "            X_train_cv,\n",
        "            y_train_cv,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_data=(X_test_cv, y_test_cv),\n",
        "            callbacks=[early_stop],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        y_pred_cv_prob = model.predict(X_test_cv)\n",
        "        y_pred_cv = (y_pred_cv_prob >= 0.5).astype(int)\n",
        "\n",
        "        # Compute recall and F1 score\n",
        "        recall = recall_score(y_test_cv, y_pred_cv, average='macro', zero_division=0)\n",
        "        f1 = f1_score(y_test_cv, y_pred_cv, average='macro', zero_division=0)\n",
        "\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold {fold}: {model_builder.__name__} Recall = {recall:.4f}, F1-Score = {f1:.4f}\")\n",
        "\n",
        "    avg_recall = np.mean(recall_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    return {'Recall': avg_recall, 'F1': avg_f1}\n",
        "\n",
        "def evaluate_classifier(clf, clf_name, X_train, y_train, X_test, y_test, label_names):\n",
        "    \"\"\"\n",
        "    Train the classifier, make predictions, and evaluate performance.\n",
        "\n",
        "    Parameters:\n",
        "    - clf: The classifier to evaluate.\n",
        "    - clf_name (str): Name of the classifier.\n",
        "    - X_train (sparse matrix or ndarray): Training feature matrix.\n",
        "    - y_train (ndarray): Training labels.\n",
        "    - X_test (sparse matrix or ndarray): Testing feature matrix.\n",
        "    - y_test (ndarray): Testing labels.\n",
        "    - label_names (list): List of label names.\n",
        "\n",
        "    Returns:\n",
        "    - list of dicts: List containing evaluation metrics for each label.\n",
        "    \"\"\"\n",
        "    print(f\"\\n===== Evaluating {clf_name} =====\")\n",
        "\n",
        "    # Fit the model\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = clf.predict(X_test)\n",
        "\n",
        "    # Calculate Hamming Loss\n",
        "    hamming_loss_value = hamming_loss(y_test, predictions)\n",
        "\n",
        "    # Calculate metrics for each label\n",
        "    metrics = []\n",
        "    n_labels = y_test.shape[1]\n",
        "\n",
        "    for label_idx in range(n_labels):\n",
        "        y_true_label = y_test[:, label_idx]\n",
        "        y_pred_label = predictions[:, label_idx]\n",
        "\n",
        "        recall = recall_score(y_true_label, y_pred_label, zero_division=0)\n",
        "        f1 = f1_score(y_true_label, y_pred_label, zero_division=0)\n",
        "\n",
        "        metrics.append({\n",
        "            'Model': clf_name,\n",
        "            'Label': label_names[label_idx],\n",
        "            'Recall': recall,\n",
        "            'F1': f1,\n",
        "            'Hamming Loss': hamming_loss_value\n",
        "        })\n",
        "\n",
        "    # Print Hamming Loss\n",
        "    print(f\"Hamming Loss for {clf_name}: {hamming_loss_value}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def evaluate_deep_learning_model(model, X_test, y_test, model_name, label_names):\n",
        "    \"\"\"\n",
        "    Evaluate the Deep Learning model on the test set.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained Keras model.\n",
        "    - X_test (sparse matrix or ndarray): Testing feature matrix.\n",
        "    - y_test (ndarray): Testing labels.\n",
        "    - model_name (str): Name of the model for reporting.\n",
        "    - label_names (list): List of label names.\n",
        "\n",
        "    Returns:\n",
        "    - list of dicts: List containing evaluation metrics for each label.\n",
        "    \"\"\"\n",
        "    print(f\"\\n===== Evaluating {model_name} Model =====\")\n",
        "\n",
        "    if hasattr(X_test, \"toarray\"):\n",
        "        X_test = X_test.toarray()\n",
        "    # Make predictions\n",
        "    y_pred_prob = model.predict(X_test)\n",
        "    y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "    # Calculate Hamming Loss\n",
        "    hamming_loss_value = hamming_loss(y_test, y_pred)\n",
        "\n",
        "    # Calculate metrics for each label\n",
        "    metrics = []\n",
        "    n_labels = y_test.shape[1]\n",
        "\n",
        "    for label_idx in range(n_labels):\n",
        "        y_true_label = y_test[:, label_idx]\n",
        "        y_pred_label = y_pred[:, label_idx]\n",
        "\n",
        "        recall = recall_score(y_true_label, y_pred_label, zero_division=0)\n",
        "        f1 = f1_score(y_true_label, y_pred_label, zero_division=0)\n",
        "\n",
        "        metrics.append({\n",
        "            'Model': model_name,\n",
        "            'Label': label_names[label_idx],\n",
        "            'Recall': recall,\n",
        "            'F1': f1,\n",
        "            'Hamming Loss': hamming_loss_value\n",
        "        })\n",
        "\n",
        "    # Print Hamming Loss\n",
        "    print(f\"Hamming Loss for {model_name} Model: {hamming_loss_value}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def build_deep_learning_model(input_dim, output_dim):\n",
        "    \"\"\"\n",
        "    Build and compile a Multilayer Perceptron (MLP) model for multi-label classification.\n",
        "\n",
        "    Parameters:\n",
        "    - input_dim (int): Number of input features.\n",
        "    - output_dim (int): Number of output labels.\n",
        "\n",
        "    Returns:\n",
        "    - model (Sequential): Compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(output_dim, activation='sigmoid'))  # Sigmoid for multi-label classification\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_cnn_model():\n",
        "    \"\"\"\n",
        "    Build and compile a CNN model for text classification.\n",
        "\n",
        "    Returns:\n",
        "    - model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    global vocab_size, embedding_dim, max_len, output_dim\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "    model.add(Conv1D(128, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(output_dim, activation='sigmoid'))  # Sigmoid activation for multi-label classification\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# ====================================\n",
        "# Main Execution\n",
        "# ====================================\n",
        "\n",
        "def main(data_type='Unbalanced'):\n",
        "    \"\"\"\n",
        "    Main function to execute data processing, model training, evaluation, and visualization.\n",
        "\n",
        "    Parameters:\n",
        "    - data_type (str): Type of data to process ('Unbalanced' or 'Balanced').\n",
        "    \"\"\"\n",
        "    # ----------------------------\n",
        "    # Load Data\n",
        "    # ----------------------------\n",
        "    csv_path = \"dataset.csv\"\n",
        "    try:\n",
        "        if data_type == 'Balanced':\n",
        "            X_train_df, X_test_df, y_train_df, y_test_df = load_data_balanced(csv_path)\n",
        "        else:\n",
        "            X_train_df, X_test_df, y_train_df, y_test_df = load_data(csv_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Visualizations\n",
        "    # ----------------------------\n",
        "    visualize_description_length(X_train_df, data_type)\n",
        "    visualize_class_distribution(y_train_df, y_test_df, data_type)\n",
        "    visualize_correlation_matrix(y_train_df, data_type)\n",
        "    visualize_label_frequency(y_train_df, data_type)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Word Clouds and Vocabulary Collection\n",
        "    # ----------------------------\n",
        "    vocab_set = set()  # Initialize an empty set to collect unique words\n",
        "\n",
        "    for label in LABELS:\n",
        "        top_words = visualize_word_cloud(X_train_df, y_train_df, label)\n",
        "        vocab_set.update(top_words)\n",
        "\n",
        "    wordcloud_vocab = list(vocab_set)\n",
        "    print(f\"\\nTotal unique words collected from word clouds: {len(wordcloud_vocab)}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Prepare Data with Vocabulary\n",
        "    # ----------------------------\n",
        "    top_k = 50  # Number of top features to select based on Chi-Square scores\n",
        "    try:\n",
        "        X_train_tfidf, X_test_tfidf, selected_features, chi2_scores_max, vector = prepare_data(\n",
        "            X_train_df, X_test_df, y_train_df, top_k=top_k, vocabulary=wordcloud_vocab\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data preparation: {e}\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Check for Selected Features\n",
        "    # ----------------------------\n",
        "    if X_train_tfidf.shape[1] == 0:\n",
        "        raise ValueError(\"No features were selected. Consider reducing the 'top_k' parameter or using alternative feature selection methods.\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Convert Labels to NumPy Arrays\n",
        "    # ----------------------------\n",
        "    y_train_np = y_train_df.to_numpy()\n",
        "    y_test_np = y_test_df.to_numpy()\n",
        "\n",
        "    # Get label names\n",
        "    label_names = y_test_df.columns.tolist()\n",
        "\n",
        "    # Plot Top Features\n",
        "    plot_top_features(selected_features, chi2_scores_max, data_type, top_k_plot=20)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Define Classifiers\n",
        "    # ----------------------------\n",
        "    clf1 = ClassifierChain(MultinomialNB())\n",
        "    clf2 = ClassifierChain(LogisticRegression(max_iter=10000))\n",
        "    clf3 = ClassifierChain(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "    # ----------------------------\n",
        "    # Cross-Validation for Traditional Models\n",
        "    # ----------------------------\n",
        "    meth_cv = []\n",
        "    for clf, model_name in zip([clf1, clf2, clf3], ['MultinomialNB', 'LogisticRegression', 'RandomForest']):\n",
        "        print(f\"\\n===== Cross-Validating {model_name} =====\")\n",
        "        cv_scores = cross_validation_score_multilabel(clf, X_train_tfidf, y_train_np)\n",
        "        meth_cv.append({'Model': model_name, 'Recall': cv_scores['Recall'], 'F1': cv_scores['F1']})\n",
        "    meth_cv = pd.DataFrame(meth_cv)\n",
        "    print(\"\\nCross-validation results:\")\n",
        "    print(meth_cv[['Model', 'Recall', 'F1']])\n",
        "\n",
        "    # ----------------------------\n",
        "    # Evaluate Classifiers on Test Set\n",
        "    # ----------------------------\n",
        "    results_nb = evaluate_classifier(clf1, 'MultinomialNB', X_train_tfidf, y_train_np, X_test_tfidf, y_test_np, label_names)\n",
        "    results_lr = evaluate_classifier(clf2, 'LogisticRegression', X_train_tfidf, y_train_np, X_test_tfidf, y_test_np, label_names)\n",
        "    results_rf = evaluate_classifier(clf3, 'RandomForest', X_train_tfidf, y_train_np, X_test_tfidf, y_test_np, label_names)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Cross-Validation for Deep Learning Model\n",
        "    # ----------------------------\n",
        "    print(\"\\n===== Training and Evaluating Deep Learning Model via Cross-Validation =====\")\n",
        "    deep_learning_cv_scores = cross_validation_score_deep_learning(\n",
        "        lambda: build_deep_learning_model(X_train_tfidf.shape[1], y_train_np.shape[1]),\n",
        "        X_train_tfidf.toarray(), y_train_np, n_splits=10, epochs=100, batch_size=16\n",
        "    )\n",
        "    print(\"\\nDeep Learning Cross-validation results:\")\n",
        "    print(f\"Recall: {deep_learning_cv_scores['Recall']:.4f}\")\n",
        "    print(f\"F1-score: {deep_learning_cv_scores['F1']:.4f}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Train Deep Learning Model on Entire Training Set\n",
        "    # ----------------------------\n",
        "    print(\"\\n===== Training Deep Learning Model on Entire Training Set =====\")\n",
        "    deep_learning_model = build_deep_learning_model(input_dim=X_train_tfidf.shape[1], output_dim=y_train_np.shape[1])\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    deep_learning_model.fit(\n",
        "        X_train_tfidf.toarray(),\n",
        "        y_train_np,\n",
        "        epochs=100,\n",
        "        batch_size=16,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # ----------------------------\n",
        "    # Evaluate Deep Learning Model on Test Set\n",
        "    # ----------------------------\n",
        "    results_dl = evaluate_deep_learning_model(deep_learning_model, X_test_tfidf, y_test_np, 'MLP', label_names)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Prepare Data for CNN\n",
        "    # ----------------------------\n",
        "    X_train_dl, X_test_dl, tokenizer = prepare_data_for_deep_learning(\n",
        "        X_train_df['report'],\n",
        "        X_test_df['report'],\n",
        "        max_words=5000,\n",
        "        max_len=100\n",
        "    )\n",
        "\n",
        "    # Parameters for CNN\n",
        "    global vocab_size, embedding_dim, max_len, output_dim\n",
        "    vocab_size = min(len(tokenizer.word_index) + 1, 5000)\n",
        "    embedding_dim = 100\n",
        "    max_len = X_train_dl.shape[1]\n",
        "    output_dim = y_train_np.shape[1]\n",
        "\n",
        "    # ----------------------------\n",
        "    # Cross-Validation for CNN Model\n",
        "    # ----------------------------\n",
        "    print(\"\\n===== Training and Evaluating CNN Model via Cross-Validation =====\")\n",
        "    cnn_cv_scores = cross_validation_score_deep_learning(\n",
        "        build_cnn_model, X_train_dl, y_train_np, n_splits=10, epochs=10, batch_size=32\n",
        "    )\n",
        "    print(\"\\nCNN Cross-validation results:\")\n",
        "    print(f\"Recall: {cnn_cv_scores['Recall']:.4f}\")\n",
        "    print(f\"F1-score: {cnn_cv_scores['F1']:.4f}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Train CNN Model on Entire Training Set\n",
        "    # ----------------------------\n",
        "    print(\"\\n===== Training CNN Model on Entire Training Set =====\")\n",
        "    cnn_model = build_cnn_model()\n",
        "\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    cnn_model.fit(\n",
        "        X_train_dl,\n",
        "        y_train_np,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # ----------------------------\n",
        "    # Evaluate CNN Model on Test Set\n",
        "    # ----------------------------\n",
        "    results_cnn = evaluate_deep_learning_model(cnn_model, X_test_dl, y_test_np, 'CNN', label_names)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Combine Results\n",
        "    # ----------------------------\n",
        "    combined_results = results_nb + results_lr + results_rf + results_dl + results_cnn\n",
        "    df_results = pd.DataFrame(combined_results)\n",
        "\n",
        "    # Convert 'Hamming Loss' to numeric\n",
        "    df_results['Hamming Loss'] = pd.to_numeric(df_results['Hamming Loss'], errors='coerce')\n",
        "\n",
        "    # ----------------------------\n",
        "    # Visualization of Results\n",
        "    # ----------------------------\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Box plot for F1 Score Distribution\n",
        "    visualize_f1_scores(df_results,data_type)\n",
        "\n",
        "    # Box plot comparing Recall, F1-score, and Hamming Loss across all models\n",
        "    visualize_all_metrics_boxplot(df_results, data_type)\n",
        "\n",
        "    # Bar plots for Multinomial Naive Bayes metrics\n",
        "    visualize_nb_metrics(df_results, data_type)\n",
        "\n",
        "    print(\"\\nAll visualization processes completed successfully. Plots have been saved.\")\n",
        "\n",
        "# ====================================\n",
        "# Execute Scripts\n",
        "# ====================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\nProcessing with Unbalanced Data.\")\n",
        "    main(data_type='Unbalanced')\n",
        "    print(\"\\n---------------------------------------------------------\")\n",
        "    print(\"\\nProcessing with Balanced Data.\")\n",
        "    main(data_type='Balanced')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install iterative-stratification\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsthNQaZ_zZo",
        "outputId": "7ad18770-b319-422b-9b46-84ef6d28df33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.5.0)\n",
            "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.9\n"
          ]
        }
      ]
    }
  ]
}